import pandas as pd

from pipelines.helpers import gdalwarp_bounds

configfile: "config.yml"

# import our dataset specific rules
module aqueduct:
    snakefile: "pipelines/aqueduct/rules.smk"
    config: config
use rule * from aqueduct as aqueduct_*
ruleorder: aqueduct_POST_metadata_to_backend > POST_metadata_to_backend

module esa_land_cover:
    snakefile: "pipelines/esa_land_cover/rules.smk"
    config: config
use rule * from esa_land_cover as esa_land_cover_*
ruleorder: esa_land_cover_download_300m_2020_from_CDS > clip_raster

module iris:
    snakefile: "pipelines/iris/rules.smk"
    config: config
use rule * from iris as iris_*

module jrc_pop:
    snakefile: "pipelines/jrc_pop/rules.smk"
    config: config
use rule * from jrc_pop as jrc_pop_*
ruleorder: jrc_pop_clip_and_reproject_raster > clip_raster

module storm:
    snakefile: "pipelines/storm/rules.smk"
    config: config
use rule * from storm as storm_*

# datasets the `all` target rule will expand to
ALL_DATASETS = (
    "aqueduct",
    "esa_land_cover",
    #"exposure_nature",
    #"gem_exposure",
    #"gem_earthquake",
    #"ghsl_buildings",
    "iris",
    #"isimp_drought",
    #"isimp_extreme_heat",
    "jrc_pop",
    "storm",
    #"traveltime_to_healthcare",
)

# prefix any shell blocks with these statements
# - create variables from env file and export
# - print commands to stdout during execution
shell.prefix(f"set -a && source ../envs/{config['environment']}/.etl.env && set +a; set -x ;")

wildcard_constraints:
    KEY="[^/]+",
    DATASET="[^/]+",
    EPOCH="\d+",
    RP="\d+",
    SSP="constant|ssp\d",


rule set_zero_to_no_data:
    input:
        "raster/raw/{DATASET}/{KEY}.tif"
    output:
        temp("raster/no_data/{DATASET}/{KEY}.tif")
    resources:
        disk_mb=3000,
        mem_mb=10000,
    priority:
        70,
    shell:
        """
        NODATA=$(gdalinfo "{input}" -json | jq .bands[0].noDataValue)

        # handle case of NODATA == nan - the JSON output of gdalinfo will change
        # nan to "NaN" so we need to reverse that for gdal_calc.py
        if [ "$NODATA" == '"NaN"' ]
        then
          NODATA=nan
        fi

        if [ "$NODATA" == 'null' ]
        then
          NODATA=nan
        fi

        # replace zeros with NoData value
        gdal_calc.py \
          --quiet \
          -A "{input}" \
          --co="COMPRESS=LZW" \
          --outfile="{output}" \
          --overwrite \
          --calc="numpy.where(A<=0,$NODATA,A)" \
          --NoDataValue=$NODATA \
          --hideNoData
        """


rule clip_raster:
    """
    Clip raster extent to window defined by `raster_bounds` in config.
    """
    input:
        "raster/no_data/{DATASET}/{KEY}.tif"
    output:
        temp("raster/clip/{DATASET}/{KEY}.tif")
    params:
        bounds = gdalwarp_bounds(config["raster_bounds"])
    resources:
        disk_mb=3000,
        mem_mb=10000,
    priority:
        80,
    shell:
        """
        gdalwarp \
            -co "COMPRESS=LZW" \
            -te {params.bounds} \
            -of GTiff \
            {input} \
            {output}
        """


rule cloud_optimise_raster:
    """
    Use terracotta to cloud optimise rasters.
    """
    input:
        "raster/clip/{DATASET}/{KEY}.tif"
    output:
        "raster/cog/{DATASET}/{KEY}.tif",
    resources:
        disk_mb=100,
        mem_mb=2000,
    threads: 2
    priority:
        90,
    shell:
        """
        terracotta optimize-rasters \
            -o $(dirname {output}) \
            --overwrite \
            --reproject \
            --nproc {threads} \
            --resampling-method nearest \
            {input}
        """


rule POST_metadata_to_backend:
    """
    Requires the `backend` and postgreSQL `db` services to be running.
    """
    input:
        ingest_flag = "pipelines/{DATASET}/ingested_to_mysql.flag",
        metadata = "pipelines/{DATASET}/metadata.json",
    output:
        flag = "pipelines/{DATASET}/posted_to_backend.flag"
    shell:
        """
        # N.B. 4XX responses result in a zero-valued httpie exit status
        http POST http://$BE_HOST:$BE_PORT/tiles/sources x-token:$BE_API_TOKEN < {input.metadata}

        touch {output.flag}
        """


rule all:
    """
    Target rule to fetch, process and ingest all registered datasets.
    """
    input:
        lambda x: expand("pipelines/{dataset}/posted_to_backend.flag", dataset=ALL_DATASETS)