import os
import subprocess

import pandas


# Example config
# - override by running:
#    snakemake --cores=1 --configfile=config_local.yml all
configfile: "config.yml"

# Read further configuration data from CSV
hazard_layers = pandas.read_csv(config["hazard_layers"])
network_layers = pandas.read_csv(config["network_layers"])
network_tilelayers = pandas.read_csv(config["network_tilelayers"])
damage_rp_files = pandas.read_csv(config["damage_rp_files"])
damage_exp_files = pandas.read_csv(config["damage_exp_files"])


wildcard_constraints:
    layer="[^/]+",


# Run all file-base jobs, after load_to_database
rule all:
    input:
        expand("../tileserver/vector/data/{layer}.mbtiles", layer=network_tilelayers.layer)
        # expand("../tileserver/raster/data/{slug}.tif", slug=hazard_layers.slug)


# Prerequisite, not fully traced by file dependency
#   snakemake --cores=all load_to_database
rule load_to_database:
    input:
        expand("logs/network/{layer}.txt", layer=network_layers.ref),
        expand("logs/damages_rp/{layer}.txt", layer=damage_rp_files.ref),
        expand("logs/damages_exp/{layer}.txt", layer=damage_exp_files.ref),


rule networks_to_db:
    """Read from source directory to database
    """
    output:
        "logs/network/{layer}.txt",
    script:
        "./scripts/networks_to_db.py"


rule damages_rp_to_db:
    input:
        damage=f"{config['analysis_data_dir']}/results/direct_damages_summary_uids/{{layer}}_damages.parquet",
        exposure=f"{config['analysis_data_dir']}/results/direct_damages_summary_uids/{{layer}}_exposures.parquet",
        loss=f"{config['analysis_data_dir']}/results/direct_damages_summary_uids/{{layer}}_losses.parquet",
    output:
        "logs/damages_rp/{layer}.txt",
    script:
        "./scripts/damages_rp_to_db.py"


rule damages_exp_to_db:
    input:
        expected=f"{config['analysis_data_dir']}/results/direct_damages_summary_uids/{{layer}}_EAD_EAEL.parquet",
    output:
        "logs/damages_exp/{layer}.txt",
    script:
        "./scripts/damages_exp_to_db.py"


rule db_to_geojsonseq:
    """Load from database to GeoJSONSeq
    """
    output:
        "vector/{layer}.geojsonl",
    script:
        "scripts/db_to_geojsonseq.py"


rule geojsonseq_to_vector_tiles:
    input:
        "vector/{layer}.geojsonl",
    output:
        "../tileserver/vector/data/{layer}.mbtiles",
    run:
        layer = get_tilelayer(wildcards.layer)
        if layer.spatial_type == "line":
            options = [
                "--drop-densest-as-needed",
                "--minimum-zoom=3",
                "--maximum-zoom=15",
            ]
        elif layer.spatial_type == "polygon":
            options = [
                "--drop-densest-as-needed",
                "--minimum-zoom=3",
                "--maximum-zoom=15",
            ]
        else:
            options = ["-zg"]

        subprocess.run(
            [
                "tippecanoe",
                "--use-attribute-for-id=uid",
                "--read-parallel",
                f"--output={output}",
                f"--layer={wildcards.layer}",
            ]
            + options
            + ["--force", f"{input}"]
        )


rule tileserver_vector_config:
    input:
        expand("../tileserver/vector/data/{layer}.mbtiles", layer=network_tilelayers.layer),
        "../tileserver/vector/data/regions_parish.mbtiles",
        "../tileserver/vector/data/regions_parish_labels.mbtiles",
        "../tileserver/vector/data/regions_enumeration.mbtiles",
        "../tileserver/vector/data/regions_enumeration_labels.mbtiles",
    output:
        "../tileserver/vector/config.json"
    run:
        with open(str(output), 'w') as fh:
            tileserver_vector_config = {
                "options": {
                    "paths": {
                        "root": "",
                        "fonts": "fonts",
                        "mbtiles": "data"
                    }
                },
                "styles": {}
            }
            data = {}
            for fname in sorted(input):
                fname = os.path.basename(fname)
                data[fname.replace(".mbtiles","")] = {"mbtiles": fname}
            tileserver_vector_config["data"] = data

            json.dump(tileserver_vector_config, fh, indent=2)


rule raster_zero_nodata:
    output:
        temp("nodata-{slug}.tif"),
    shell:
        """
        NODATA=$(gdalinfo "{input}" -json | jq .bands[0].noDataValue)

        # handle case of NODATA == nan - the JSON output of gdalinfo will change
        # nan to "NaN" so we need to reverse that for gdal_calc.py
        if [ "$NODATA" == '"NaN"' ]# then
          NODATA=nan
        fi

        # replace zeros with NoData value
        gdal_calc.py \
          -A "{input}" \
          --outfile="{output}" \
          --overwrite \
          --calc="numpy.where(A==0,$NODATA,A)" \
          --hideNoData \
          --NoDataValue=$NODATA
        """


rule raster_to_cog:
    input:
        rules.raster_zero_nodata.output,
    output:
        "../tileserver/raster/data/{slug}.tif",
    shell:
        """
        # translate to Cloud-Optimised GeoTIFF
        # TODO tune COG conversion parameters or try terracotta optimize-rasters
        gdal_translate "{input}" "{output}" -of COG
        """
