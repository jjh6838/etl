import os
import subprocess
import shutil
from typing import List
import shutil

import geopandas
import pandas


def disk_free_mb(wildcards, attempt):
    _, _, free = shutil.disk_usage("/") 
    return int(free / 1000000)

# Load Config
configfile: "/opt/etl/pipelines/storm/config.yml"


# Read further configuration data from CSV
hazard_layers = pandas.read_csv(config["hazard_layers"])


wildcard_constraints:
    layer="[^/]+",

# Run all file-base jobs, after load_to_database
rule all:
    input:
        expand("/opt/tileserver/raster/data/storm/{layer}.tif", layer=hazard_layers.key),

rule raster_temp_file:
    output:
        "/opt/etl/raster/link/{layer}.tif",
    resources:
        disk_mb=100
    priority:
        60,
    run:
        layer = get_hazard_layer(wildcards.layer, hazard_layers)
        print (layer.path, output)
        shutil.copyfile(
            layer.path,
            str(output)
        )

def get_hazard_layer(layer_name, hazard_layers):
    try:
        print ("getting hazard layer: ", layer_name)
        return hazard_layers[hazard_layers.key == layer_name].iloc[0]
    except IndexError as e:
        print(f"Could not find {layer_name} in hazard layers.")
        raise e
    finally:
        print ("got hazard layer")

rule raster_zero_nodata:
    input:
        rules.raster_temp_file.output,
    output:
        "/opt/etl/raster/nodata/{layer}.tif",
    resources:
        disk_mb=3000
    priority:
        70,
    shell:
        """
        NODATA=$(gdalinfo "{input}" -json | jq .bands[0].noDataValue)

        # handle case of NODATA == nan - the JSON output of gdalinfo will change
        # nan to "NaN" so we need to reverse that for gdal_calc.py
        if [ "$NODATA" == '"NaN"' ]
        then
          NODATA=nan
        fi

        if [ "$NODATA" == 'null' ]
        then
          NODATA=nan
        fi

        if [ "$NODATA" == '3.4028235e+38' ]
        then
          NODATA=nan
        fi

        # replace zeros with NoData value
        gdal_calc.py \
          --quiet \
          -A "{input}" \
          --outfile="{output}" \
          --overwrite \
          --calc="numpy.where(A<=0,$NODATA,A)" \
          --NoDataValue=$NODATA \
          --hideNoData && \
        rm {input}
        """

rule raster_clip_ns:
    input:
        rules.raster_zero_nodata.output,
    output:
        "/opt/etl/raster/clip/{layer}.tif",
    resources:
        disk_mb=3000
    priority:
        80,
    shell:
        """
        gdal_translate -projwin -175 84 175 -84 -of GTiff {input} {output} && \
        rm {input}
        """

rule raster_to_cog:
    input:
        rules.raster_clip_ns.output,
    output:
        "/opt/etl/raster/cog/{layer}.tif",
    resources:
        disk_mb=100
    priority:
        90,
    shell:
        """
        # translate to Cloud-Optimised GeoTIFF
        #
        # could use gdalwarp directly - options chosen to match (reasonably
        # closely) the output of `terracotta optimize-rasters`:
        #
        # gdalwarp "{input}" "{output}" \
        #     -t_srs "EPSG:3857" \
        #     -r near \
        #     -of COG \
        #     -co COMPRESS=DEFLATE \
        #     -co BLOCKSIZE=256
        #
        terracotta optimize-rasters \
            -o /opt/etl/raster/cog \
            --overwrite \
            --reproject \
            --nproc -1 \
            --resampling-method nearest \
            {input} && \
        rm {input}
        """

rule raster_cleanup:
    input:
        rules.raster_to_cog.output,
    output:
        "/opt/tileserver/raster/data/storm/{layer}.tif",
    resources:
        disk_mb=100
    priority:
        100,
    shell:
        """
        mv {input} {output}
        """
        